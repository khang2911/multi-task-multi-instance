{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from torch_geometric.nn import GCNConv\n",
    "from utils import create_numeric_mapping\n",
    "from layers import ListModule, PrimaryCapsuleLayer, Attention, SecondaryCapsuleLayer, margin_loss\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--theta'], dest='theta', nargs=None, const=None, default=0.1, type=<class 'float'>, choices=None, help='Reconstruction loss weight. Default is 0.1.', metavar=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description = \"Run CapsGNN.\")\n",
    "\n",
    "parser.add_argument(\"--train-graph-folder\",\n",
    "                    nargs = \"?\",\n",
    "                    default = \"../input/train/\",\n",
    "                help = \"Training graphs folder.\")\n",
    "\n",
    "parser.add_argument(\"--test-graph-folder\",\n",
    "                    nargs = \"?\",\n",
    "                    default = \"../input/test/\",\n",
    "                help = \"Testing graphs folder.\")\n",
    "\n",
    "parser.add_argument(\"--prediction-path\",\n",
    "                    nargs = \"?\",\n",
    "                    default = \"../output/watts_predictions.csv\",\n",
    "                help = \"Path to store the predicted graph labels.\")\n",
    "\n",
    "parser.add_argument(\"--epochs\",\n",
    "                    type = int,\n",
    "                    default = 1,\n",
    "                help = \"Number of training epochs. Default is 100.\")\n",
    "\n",
    "parser.add_argument(\"--batch-size\",\n",
    "                    type = int,\n",
    "                    default = 32,\n",
    "                help = \"Number of graphs processed per batch. Default is 32.\")\n",
    "\n",
    "parser.add_argument(\"--gcn-filters\",\n",
    "                    type = int,\n",
    "                    default = 20,\n",
    "                help = \"Number of Graph Convolutional filters. Default is 20.\")\n",
    "\n",
    "parser.add_argument(\"--gcn-layers\",\n",
    "                    type = int,\n",
    "                    default = 2,\n",
    "                help = \"Number of Graph Convolutional Layers. Default is 2.\")\n",
    "\n",
    "parser.add_argument(\"--inner-attention-dimension\",\n",
    "                    type = int,\n",
    "                    default = 20,\n",
    "                help = \"Number of Attention Neurons. Default is 20.\")\n",
    "\n",
    "parser.add_argument(\"--capsule-dimensions\",\n",
    "                    type = int,\n",
    "                    default = 8,\n",
    "                help = \"Capsule dimensions. Default is 8.\")\n",
    "\n",
    "parser.add_argument(\"--number-of-capsules\",\n",
    "                    type = int,\n",
    "                    default = 8,\n",
    "                help = \"Number of capsules per layer. Default is 8.\")\n",
    "\n",
    "parser.add_argument(\"--weight-decay\",\n",
    "                    type = float,\n",
    "                    default = 10**-6,\n",
    "                help = \"Weight decay. Default is 10^-6.\")\n",
    "\n",
    "parser.add_argument(\"--learning-rate\",\n",
    "                    type = float,\n",
    "                    default = 0.001,\n",
    "                help = \"Learning rate. Default is 0.01.\")\n",
    "\n",
    "parser.add_argument(\"--lambd\",\n",
    "                    type = float,\n",
    "                    default = 0.5,\n",
    "                help = \"Loss combination weight. Default is 0.5.\")\n",
    "\n",
    "parser.add_argument(\"--theta\",\n",
    "                    type = float,\n",
    "                    default = 0.1,\n",
    "                help = \"Reconstruction loss weight. Default is 0.1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_known_args()[0]\n",
    "model = CapsGNNTrainer(args)\n",
    "model.fit()\n",
    "model.score()\n",
    "model.save_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of themodel described in the following paper:\n",
    "    https://openreview.net/forum?id=Byl8BnRcYm\n",
    "    \"\"\"\n",
    "    def __init__(self, args, number_of_features, number_of_targets):\n",
    "        super(CapsGNN, self).__init__()\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :param number_of_features: Number of vertex features.\n",
    "        :param number_of_targets: Number of classes.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.number_of_features = number_of_features\n",
    "        self.number_of_targets = number_of_targets\n",
    "        self._setup_layers()\n",
    "\n",
    "    def _setup_base_layers(self):\n",
    "        \"\"\"\n",
    "        Creating GCN layers.\n",
    "        \"\"\"\n",
    "        self.base_layers = [GCNConv(self.number_of_features, self.args.gcn_filters)]\n",
    "        for layer in range(self.args.gcn_layers-1):\n",
    "            self.base_layers.append(GCNConv( self.args.gcn_filters, self.args.gcn_filters))\n",
    "        self.base_layers = ListModule(*self.base_layers)\n",
    "\n",
    "    def _setup_primary_capsules(self):\n",
    "        \"\"\"\n",
    "        Creating primary capsules.\n",
    "        \"\"\"\n",
    "        self.first_capsule = PrimaryCapsuleLayer(in_units = self.args.gcn_filters, in_channels = self.args.gcn_layers, num_units = self.args.gcn_layers, capsule_dimensions = self.args.capsule_dimensions)\n",
    "\n",
    "    def _setup_attention(self):\n",
    "        \"\"\"\n",
    "        Creating attention layer.\n",
    "        \"\"\"\n",
    "        self.attention = Attention(self.args.gcn_layers* self.args.capsule_dimensions, self.args.inner_attention_dimension)\n",
    "\n",
    "    def _setup_graph_capsules(self):\n",
    "        \"\"\"\n",
    "        Creating graph capsules.\n",
    "        \"\"\"\n",
    "        self.graph_capsule = SecondaryCapsuleLayer(self.args.gcn_layers, self.args.capsule_dimensions, self.args.number_of_capsules, self.args.capsule_dimensions)\n",
    "\n",
    "    def _setup_class_capsule(self):\n",
    "        \"\"\"\n",
    "        Creating class capsules.\n",
    "        \"\"\"\n",
    "        self.class_capsule =  SecondaryCapsuleLayer(self.args.capsule_dimensions,self.args.number_of_capsules, self.number_of_targets, self.args.capsule_dimensions)\n",
    "\n",
    "    def _setup_reconstruction_layers(self):\n",
    "        \"\"\"\n",
    "        Creating histogram reconstruction layers.\n",
    "        \"\"\"\n",
    "        self.reconstruction_layer_1 = torch.nn.Linear(self.number_of_targets*self.args.capsule_dimensions, int((self.number_of_features * 2) / 3))\n",
    "        self.reconstruction_layer_2 = torch.nn.Linear(int((self.number_of_features * 2) / 3), int((self.number_of_features * 3) / 2))\n",
    "        self.reconstruction_layer_3 = torch.nn.Linear(int((self.number_of_features * 3) / 2), self.number_of_features)\n",
    "\n",
    "    def _setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating layers of model.\n",
    "        1. GCN layers.\n",
    "        2. Primary capsules.\n",
    "        3. Attention\n",
    "        4. Graph capsules.\n",
    "        5. Class capsules.\n",
    "        6. Reconstruction layers.\n",
    "        \"\"\"\n",
    "        self._setup_base_layers()\n",
    "        self._setup_primary_capsules()\n",
    "        self._setup_attention()\n",
    "        self._setup_graph_capsules()\n",
    "        self._setup_class_capsule()\n",
    "        self._setup_reconstruction_layers()\n",
    "\n",
    "    def calculate_reconstruction_loss(self, capsule_input, features):\n",
    "        \"\"\"\n",
    "        Calculating the reconstruction loss of the model.\n",
    "        :param capsule_input: Output of class capsule.\n",
    "        :param features: Feature matrix.\n",
    "        :return reconstrcution_loss: Loss of reconstruction.\n",
    "        \"\"\"\n",
    "\n",
    "        v_mag = torch.sqrt((capsule_input**2).sum(dim=1))\n",
    "        _, v_max_index = v_mag.max(dim=0)\n",
    "        v_max_index = v_max_index.data\n",
    "\n",
    "        capsule_masked = torch.autograd.Variable(torch.zeros(capsule_input.size()))\n",
    "        capsule_masked[v_max_index,:] = capsule_input[v_max_index,:]\n",
    "        capsule_masked = capsule_masked.view(1, -1)\n",
    "\n",
    "        feature_counts = features.sum(dim=0)\n",
    "        feature_counts = feature_counts/feature_counts.sum()\n",
    "\n",
    "        reconstruction_output = torch.nn.functional.relu(self.reconstruction_layer_1(capsule_masked))\n",
    "        reconstruction_output = torch.nn.functional.relu(self.reconstruction_layer_2(reconstruction_output))\n",
    "        reconstruction_output = torch.softmax(self.reconstruction_layer_3(reconstruction_output),dim=1)\n",
    "        reconstruction_output = reconstruction_output.view(1, self.number_of_features)\n",
    "\n",
    "        reconstruction_loss = torch.sum((features-reconstruction_output)**2)\n",
    "        \n",
    "        return reconstruction_loss\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward propagation pass.\n",
    "        :param data: Dictionary of tensors with features and edges.\n",
    "        :return class_capsule_output: Class capsule outputs.\n",
    "        \"\"\"\n",
    "        features = data[\"features\"]\n",
    "        edges = data[\"edges\"]\n",
    "        hidden_representations = []\n",
    "        \n",
    "        for layer in self.base_layers:\n",
    "            features = torch.nn.functional.relu(layer(features, edges))\n",
    "            hidden_representations.append(features)\n",
    "\n",
    "        hidden_representations = torch.cat(tuple(hidden_representations))\n",
    "        hidden_representations = hidden_representations.view(1, self.args.gcn_layers, self.args.gcn_filters,-1)\n",
    "        first_capsule_output = self.first_capsule(hidden_representations)\n",
    "        first_capsule_output = first_capsule_output.view(-1,self.args.gcn_layers* self.args.capsule_dimensions)\n",
    "        rescaled_capsule_output = self.attention(first_capsule_output)\n",
    "        rescaled_first_capsule_output = rescaled_capsule_output.view(-1, self.args.gcn_layers, self.args.capsule_dimensions)\n",
    "        graph_capsule_output = self.graph_capsule(rescaled_first_capsule_output)\n",
    "        reshaped_graph_capsule_output = graph_capsule_output.view(-1, self.args.capsule_dimensions, self.args.number_of_capsules ) \n",
    "        class_capsule_output = self.class_capsule(reshaped_graph_capsule_output)\n",
    "        class_capsule_output =  class_capsule_output.view(-1, self.number_of_targets*self.args.capsule_dimensions )\n",
    "        class_capsule_output = torch.mean(class_capsule_output,dim=0).view(1,self.number_of_targets,self.args.capsule_dimensions)\n",
    "        reconstruction_loss = self.calculate_reconstruction_loss(class_capsule_output.view(self.number_of_targets,self.args.capsule_dimensions), data[\"features\"])\n",
    "        return class_capsule_output, reconstruction_loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsGNNTrainer(object):\n",
    "    \"\"\"\n",
    "    CapsGNN training and scoring.\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.setup_model()\n",
    "\n",
    "    def enumerate_unique_labels_and_targets(self):\n",
    "        \"\"\"\n",
    "        Enumerating the features and targets in order to setup weights later.\n",
    "        \"\"\"\n",
    "        print(\"\\nEnumerating feature and target values.\\n\")\n",
    "        ending = \"*.json\"\n",
    "\n",
    "        self.train_graph_paths = glob.glob(self.args.train_graph_folder+ending)\n",
    "        self.test_graph_paths = glob.glob(self.args.test_graph_folder+ending)\n",
    "    \n",
    "        graph_paths = self.train_graph_paths + self.test_graph_paths\n",
    "        \n",
    "        #print(graph_paths)\n",
    "        \n",
    "        targets = set()\n",
    "        features = set()\n",
    "        for path in tqdm(graph_paths):\n",
    "            data = json.load(open(path))\n",
    "            targets = targets.union(set([data[\"target0\"]]))\n",
    "            features = features.union(set(data[\"labels\"]))\n",
    "\n",
    "        \n",
    "        \n",
    "        self.target_map = create_numeric_mapping(targets)\n",
    "        self.feature_map = create_numeric_mapping(features)\n",
    "\n",
    "        self.number_of_features = len(self.feature_map)\n",
    "        self.number_of_targets = len(self.target_map)\n",
    "    \n",
    "    def setup_model(self):\n",
    "        \"\"\"\n",
    "        Enumerating labels and initializing a CapsGNN.\n",
    "        \"\"\"\n",
    "        self.enumerate_unique_labels_and_targets()\n",
    "        self.model = CapsGNN(self.args, self.number_of_features, self.number_of_targets)\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\"\n",
    "        Batching the graphs for training.\n",
    "        \"\"\"\n",
    "        self.batches = [self.train_graph_paths[i:i + self.args.batch_size] for i in range(0,len(self.train_graph_paths), self.args.batch_size)]\n",
    "\n",
    "    def create_data_dictionary(self, target, edges, features):\n",
    "        \"\"\"\n",
    "        Creating a data dictionary.\n",
    "        :param target: Target vector.\n",
    "        :param edges: Edge list tensor.\n",
    "        :param features: Feature tensor.\n",
    "        \"\"\"\n",
    "        to_pass_forward = dict()\n",
    "        to_pass_forward[\"target\"] = target\n",
    "        to_pass_forward[\"edges\"] = edges\n",
    "        to_pass_forward[\"features\"] = features\n",
    "        return to_pass_forward\n",
    "\n",
    "    def create_target(self, data,task):\n",
    "        \"\"\"\n",
    "        Target createn based on data dicionary.\n",
    "        :param data: Data dictionary.\n",
    "        :return : Target vector.\n",
    "        \"\"\"\n",
    "        return  torch.FloatTensor([0.0 if i != data[\"target%s\"%task] else 1.0 for i in range(self.number_of_targets)])\n",
    "\n",
    "    def create_edges(self,data):\n",
    "        \"\"\"\n",
    "        Create an edge matrix.\n",
    "        :param data: Data dictionary.\n",
    "        :return : Edge matrix.\n",
    "        \"\"\"\n",
    "        edges = [[edge[0],edge[1]] for edge in data[\"edges\"]] + [[edge[1],edge[0]] for edge in data[\"edges\"]]\n",
    "        return torch.t(torch.LongTensor(edges))\n",
    "\n",
    "    def create_features(self,data):\n",
    "        \"\"\"\n",
    "        Create feature matrix.\n",
    "        :param data: Data dictionary.\n",
    "        :return features: Matrix of features.\n",
    "        \"\"\"\n",
    "        features = np.zeros((len(data[\"labels\"]), self.number_of_features))\n",
    "        node_indices = [node for node in range(len(data[\"labels\"]))]\n",
    "        feature_indices = [self.feature_map[label] for label in data[\"labels\"].values()] \n",
    "        features[node_indices,feature_indices] = 1.0\n",
    "        features = torch.FloatTensor(features)\n",
    "        return features\n",
    "\n",
    "    def create_input_data(self, path,task):\n",
    "        \"\"\"\n",
    "        Creating tensors and a data dictionary with Torch tensors.\n",
    "        :param path: path to the data JSON.\n",
    "        :return to_pass_forward: Data dictionary.\n",
    "        \"\"\"\n",
    "        data = json.load(open(path))\n",
    "        target = self.create_target(data,task)\n",
    "        edges = self.create_edges(data)\n",
    "        features = self.create_features(data)\n",
    "        to_pass_forward = self.create_data_dictionary(target, edges, features)\n",
    "        return to_pass_forward\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Training a model on the training set.\n",
    "        \"\"\"\n",
    "        print(\"\\nTraining started.\\n\")\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate, weight_decay=self.args.weight_decay)\n",
    "\n",
    "        for epoch in tqdm(range(self.args.epochs), desc = \"Epochs: \", leave = True):\n",
    "            random.shuffle(self.train_graph_paths)\n",
    "            self.create_batches()\n",
    "            losses = 0       \n",
    "            self.steps = trange(len(self.batches), desc=\"Loss\")\n",
    "            for step in self.steps:\n",
    "                accumulated_losses = 0\n",
    "                optimizer.zero_grad()\n",
    "                batch = self.batches[step]\n",
    "                for path in batch:\n",
    "                    loss_multi = 0 \n",
    "                    for task in range(len(tasks)):\n",
    "                        data = self.create_input_data(path,task)\n",
    "                        prediction, reconstruction_loss = self.model(data)\n",
    "                        loss = margin_loss(prediction, data[\"target\"], self.args.lambd)+self.args.theta*reconstruction_loss\n",
    "                        loss_multi += loss\n",
    "                    accumulated_losses = accumulated_losses + (loss_multi/9)\n",
    "                accumulated_losses = accumulated_losses/len(batch)\n",
    "                accumulated_losses.backward()\n",
    "                optimizer.step()\n",
    "                losses = losses + accumulated_losses.item()\n",
    "                average_loss = losses/(step + 1)\n",
    "                self.steps.set_description(\"CapsGNN (Loss=%g)\" % round(average_loss,4))\n",
    "\n",
    "    def score(self):\n",
    "        \"\"\"\n",
    "        Scoring on the test set.\n",
    "        \"\"\"\n",
    "        print(\"\\n\\nScoring.\\n\")\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.hits = []\n",
    "        \n",
    "        for path in tqdm(self.test_graph_paths):\n",
    "            for task in range(9):\n",
    "                self.task_acc = []\n",
    "                data = self.create_input_data(path,task)\n",
    "                prediction, reconstruction_loss = self.model(data)\n",
    "                prediction_mag = torch.sqrt((prediction**2).sum(dim=2))\n",
    "                _, prediction_max_index = prediction_mag.max(dim=1)\n",
    "                prediction = prediction_max_index.data.view(-1).item()\n",
    "                self.predictions.append(prediction)\n",
    "                \n",
    "                self.task_acc.append(data[\"target\"][prediction]==1.0)\n",
    "                print(\"\\nAccuracy task %s: \"%task + str(round(np.mean(self.task_acc),4)))\n",
    "                \n",
    "                self.hits.append(data[\"target\"][prediction]==1.0)\n",
    "\n",
    "        print(\"\\nAccuracy: \" + str(round(np.mean(self.hits),4)))\n",
    "\n",
    "    def save_predictions(self):\n",
    "        \"\"\"\n",
    "        Saving the test set predictions.\n",
    "        \"\"\"\n",
    "        identifiers = [path.split(\"/\")[-1].strip(\".json\") for path in self.test_graph_paths]\n",
    "        out = pd.DataFrame()\n",
    "        out[\"id\"] = identifiers\n",
    "        out[\"predictions\"] = self.predictions\n",
    "        out.to_csv(self.args.prediction_path, index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
